{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5b836998108e9ee",
   "metadata": {},
   "source": [
    "# Overview of different Particle Swarm Optimisation methods on the Travelling Salesman Problem \n",
    "\n",
    "This notebook aims to run 7 different computational experiments with multiple PSO methods. The primary goal defining the design of these experiments is to analyse the effect of heuristic methods on the best solution found by PSO  across TSP instances.\n",
    "    \n",
    "## 1. Introduction\n",
    "Six different PSO algorithms have been implemented from the literature. Each PSO algorithm incorporates a specially designed local heuristic.\n",
    "\n",
    "| Algorithm                   | Description                                                                                                                                                                                                                                                                                                                                                                    |\n",
    "|-----------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Particle Swarm Optimization (PSO) | PSO is a population-based optimization technique inspired by the social behavior of bird flocking or fish schooling. Each particle represents a potential solution that is moving through the search space guided by its personal best position (pbest) and the global best position (gbest) found by the swarm.                                                                      |\n",
    "| Adaptive PSO (APSO)         | APSO introduces an adaptive inertia weight that varies during optimization that balances exploration and exploitation. The inertia weight linearly decreases the velocity from a maximum to a minimum value.                                                                                                                                 |\n",
    "| Discrete PSO (HPSO)         | HPSO handles discrete optimization problems like the Traveling Salesman Problem. It updates particle positions using a probabilistic approach based on the sigmoid function applied to the particle's velocity.                                                                                                                                                             |\n",
    "| Spatial PSO (SPSO)          | SPSO incorporates spatial information by considering each particle's neighborhood. Velocity updates are influenced by the best positions found within the particle's neighborhood.                                                                                                                                                               |\n",
    "| Differential Evolution PSO (DEPSO) | DEPSO combines Differential Evolution (DE) and PSO. It employs the DE crossover operator to generate new candidate solutions. These solutions are then integrated into the particle's position update process.                                                                                                                                                                |\n",
    "| Predator-Prey PSO (PPPSO)   | PPPSO introduces a predator particle that follows the global best position which influences the other particles' movement. Particles attempt to evade the predator while still being guided by their personal and global best positions.                                                                                                     |\n",
    "\n",
    "\n",
    "\n",
    "## 2. Literature Review\n",
    "Particle Swarm Optimization (PSO) has been successfully applied to the Traveling Salesman Problem (TSP) due to its ability to efficiently explore the search space and find near-optimal solutions. The TSP is a well-known NP-hard combinatorial optimization problem that aims to find the shortest possible route visiting each city exactly once and returning to the starting city.\n",
    "\n",
    "PSO's success in solving the TSP can be attributed to several factors:\n",
    "\n",
    "- PSO's population-based nature allows it to maintain a diverse set of solutions throughout the optimization process. This diversity helps in exploring different regions of the search space and avoids getting stuck in local optima. The particles in PSO collaborate and share information about promising solutions, enabling the algorithm to converge towards high-quality solutions.\n",
    "\n",
    "- The velocity update mechanism effectively balances exploration and exploitation. The particles are guided by their personal best positions and the global best position which allows them to explore new regions while simultaneously exploiting the knowledge gained from previous iterations. This balance is crucial in finding good solutions to the TSP, as it requires both global exploration to discover new routes and local exploitation to refine existing routes.\n",
    "\n",
    "- The simplicity and ease of implementation also make it an attractive choice for solving the TSP. The algorithm has a small number of parameters to tune, making it relatively straightforward to apply to the problem.\n",
    "\n",
    "- PSO algorithms can be easily adapted to handle the discrete nature of the TSP by using appropriate position and velocity update mechanisms, such as the swap operator or the nearest neighbor heuristic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4114aaf00a7411",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T16:18:34.562330Z",
     "start_time": "2024-04-18T16:18:34.560188Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import tqdm\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c26b00",
   "metadata": {},
   "source": [
    "## 3. Preliminary Findings\n",
    "\n",
    "First task is to benchmark the performance of different PSO variants on the Traveling Salesman Problem (TSP) without hyperparameter optimization. The  objective is to compare the convergence behavior and solution quality of the algorithms using  sensible default parameter settings.\n",
    "\n",
    "A interesting behaviour that can be observed is that for smaller instances of TSP, Stochastic hill climber(SHC) is able to outperform the PSO algorithms. The search space size influences the performance of SHC. It can be attributed to the fact that for smaller instances,\n",
    " The probability of finding the optimal solution randomly is more as sample size is smaller, and with each choice the probability of choosing an optimal city increases $\\frac{{\\text{{number of optimal cities to choose next}}}}{{\\text{{total cities left to choose from}}}}$ It is worthy to note that all PSO methods beat SHC on  the larger instance.\n",
    "\n",
    "The differential evolution outperforms every method on larger instances. This could be due to the DE formula and its design being able to efficiently balance exploration and exploitation.\n",
    "\n",
    "Let's run some experiments to improve the performance of PSO algorithms and understand more about their hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T16:20:08.169120Z",
     "start_time": "2024-04-18T16:18:34.595437Z"
    }
   },
   "outputs": [],
   "source": [
    "#from utils import *\n",
    "from utilstest import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "# Usage example\n",
    "tsp_file_path = 'a29.tsp'\n",
    "csv_file_path = 'a29.csv'\n",
    "\n",
    "tsp_instance = ProcessData(tsp_file_path, csv_file_path)\n",
    "plotter = PrettyPlotting()\n",
    "\n",
    "experiment = Experiment(tsp_instance, num_runs=2,population_size=50,max_iterations=100, w=0.9, c1=0.1,c2=0.1, w_min=0.1, w_max=0.3, neighborhood_size=2, cr=0.1, f=0.1, fear_factor=0.9)\n",
    "experiment.run_best_hyperparameter_experiments()\n",
    "\n",
    "# Usage example\n",
    "tsp_file_path = 'a280.tsp'\n",
    "csv_file_path = 'a280_distance_matrix.csv'\n",
    "\n",
    "tsp_instance = ProcessData(tsp_file_path, csv_file_path)\n",
    "plotter = PrettyPlotting()\n",
    "\n",
    "\n",
    "experiment = Experiment(tsp_instance, num_runs=2,population_size=10,max_iterations=10, w=0.9, c1=0.1,c2=0.1, w_min=0.1, w_max=0.3, neighborhood_size=2, cr=0.1, f=0.1, fear_factor=0.9)\n",
    "experiment.run_best_hyperparameter_experiments()\n",
    "\n",
    "\n",
    "# Usage example\n",
    "tsp_file_path = 'fl1400.tsp'\n",
    "csv_file_path = 'fl1400_distance_matrix.csv'\n",
    "\n",
    "tsp_instance = ProcessData(tsp_file_path, csv_file_path)\n",
    "plotter = PrettyPlotting()\n",
    "\n",
    "\n",
    "experiment = Experiment(tsp_instance, num_runs=2,population_size=10,max_iterations=10, w=0.9, c1=0.1,c2=0.1, w_min=0.1, w_max=0.3, neighborhood_size=2, cr=0.1, f=0.1, fear_factor=0.9)\n",
    "experiment.run_best_hyperparameter_experiments()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec4dc407e591eb9",
   "metadata": {},
   "source": [
    "## 4. Experiments\n",
    "\n",
    "All hyperparameters except ablated parameter are set constant.The  ablated parameter values are passed as a list to the run_\"ablated parameter\"_experiments function.\n",
    "The Goal is to find set of optimal ablated parameter out of the 4 different chocies. The experiment can run on n ablated parameter; However, Computational costs limit experimentation on larger set of values\n",
    "\n",
    "### 4.1. Experiment 1: Impact of population size on tour length\n",
    "\n",
    "#### Experiment design:\n",
    "\n",
    "\n",
    "\n",
    "| Algorithm                        | Optimal Population Size | Time Complexity    | Comments                                                                                                                                                                                                                                                   |\n",
    "|----------------------------------|-------------------------|-------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Particle Swarm Optimization (PSO) | 60                     | O(I * P * N)| PSO achieves its best performance with a population size of 60. This suggests that a moderate-sized swarm is sufficient for PSO to effectively explore the search space and find good solutions. Increasing the population size beyond 60 may not provide significant improvements and could lead to increased computational cost. |\n",
    "| Adaptive PSO (APSO)              | 100                    | O(I * P * N) | APSO benefits from a larger population size compared to standard PSO. The adaptive mechanism in APSO allows it to adjust the inertia weight based on the search progress, which may require a larger population to maintain diversity and avoid premature convergence. A population size of 100 seems to strike a good balance between exploration and exploitation for APSO. |\n",
    "| Discrete PSO (HPSO)              | 100                   | O(I * P * N)  | HPSO also performs well with a population size of 100. The discrete nature of the TSP solution space may require a larger population to effectively explore various permutations of cities. The increased population size helps HPSO to maintain a diverse set of solutions and avoid getting stuck in local optima. |\n",
    "| Spatial PSO (SPSO)               | 100                   | O(I * P * N*K)  | The optimal population size of 100 indicates that SPSO benefits from a larger population to effectively exploit the local information within neighborhoods. The increased population size allows SPSO to explore different regions of the search space while maintaining a good balance between local and global search. |\n",
    "| Differential Evolution PSO (DEPSO) | 60                    | O(I * P * N)  | DEPSO achieves its best performance with a population size of 60. That is similar to standard PSO. The differential evolution component in DEPSO helps in generating new candidate solutions which may compensate for the need for a larger population. A moderate population size of 60 seems to be sufficient for DEPSO to explore the search space effectively. The only caveat is the longer run time of the algorithm with larger population sizes |\n",
    "| Predator-Prey PSO (PPPSO)        | 100                   | O(I * P * N)   | PPPSO performs well with a population size of 100. The predator-prey dynamics introduced in PPPSO may require a larger population to maintain a balance between the predator and prey particles. The increased population size allows for more diverse interactions and helps in escaping local optima. |\n",
    "| Random Sampling                  | 20                    | O(I * N)   | Random Sampling is a simple algorithm that generates random solutions without any intelligent search strategy. The optimal population size of 20 suggests that increasing the population size beyond this point does not provide significant improvements in solution quality. Random Sampling relies on the randomness of the generated solutions rather than the population size. |\n",
    "| Stochastic Hill Climber          | 20                    | O(I * N)    | Stochastic Hill Climber  iteratively improves a single solution.Thus, The optimal population size of 20 indicates that the algorithm does not require a large population since it focuses on a single solution at a time. Increasing the population size may not have a significant impact on the performance of Stochastic Hill Climber. |\n",
    "\n",
    "The time complexity does highly depend on the implementation. For Example  Data structures like a tree(splay tree ) offer significant performance boost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68fc2c7df1fd63e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T16:24:13.354075Z",
     "start_time": "2024-04-18T16:20:08.170081Z"
    }
   },
   "outputs": [],
   "source": [
    "# Usage example\n",
    "tsp_file_path = 'a29.tsp'\n",
    "csv_file_path = 'a29.csv'\n",
    "\n",
    "tsp_instance = ProcessData(tsp_file_path, csv_file_path)\n",
    "plotter = PrettyPlotting()\n",
    "\n",
    "\n",
    "experiment = Experiment(tsp_instance, num_runs=2,population_size=50,max_iterations=100)\n",
    "experiment.run_population_size_experiments([20,60,80,100])\n",
    "\n",
    "# Usage example\n",
    "tsp_file_path = 'a280.tsp'\n",
    "csv_file_path = 'a280_distance_matrix.csv'\n",
    "\n",
    "tsp_instance = ProcessData(tsp_file_path, csv_file_path)\n",
    "plotter = PrettyPlotting()\n",
    "\n",
    "\n",
    "experiment = Experiment(tsp_instance, num_runs=2,population_size=50,max_iterations=100)\n",
    "experiment.run_population_size_experiments([20,60,80,100])\n",
    "\n",
    "# Usage example\n",
    "tsp_file_path = 'fl1400.tsp'\n",
    "csv_file_path = 'fl1400_distance_matrix.csv'\n",
    "\n",
    "tsp_instance = ProcessData(tsp_file_path, csv_file_path)\n",
    "plotter = PrettyPlotting()\n",
    "\n",
    "\n",
    "experiment = Experiment(tsp_instance, num_runs=2,population_size=50,max_iterations=100)\n",
    "experiment.run_population_size_experiments([20,60,80,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de0176e304931c6",
   "metadata": {},
   "source": [
    "### 4.2. Experiment 2: Impact of Inertia Weight on best tour length\n",
    "\n",
    "#### Experiment Design:\n",
    "\n",
    "\n",
    "| Algorithm                   | Optimal Inertia Weight | Comments                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
    "|-----------------------------|------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Particle Swarm Optimization (PSO) | 1.0                    | PSO achieves the best performance with an inertia weight of 1.0, indicating a strong emphasis on the particle's previous velocity. This allows for an extensive exploration of the search space which enables PSO to escape local optima and discover high-quality solutions. However, a high inertia weight may lead to slower convergence and increased computational cost.                                                                                          |\n",
    "| Discrete PSO (HPSO)         | 1.0                    | HPSO also performs optimally with an inertia weight of 1.0. A high inertia weight in HPSO promotes exploration, allowing particles to make larger jumps in the discrete space and diversify the search. This helps in avoiding premature convergence to suboptimal solutions.                                             |\n",
    "| Spatial PSO (SPSO)          | 0.4                    | SPSO incorporates spatial information by considering the neighborhood of each particle. The optimal inertia weight of 0.4 suggests that SPSO benefits from a lower inertia weight, which emphasizes the influence of the local best solutions within the particle's neighborhood. This allows for more focused exploitation of promising regions in the search space. The lower inertia weight helps in striking a balance between local search and global exploration which leads to faster convergence and improved solution quality. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2631c09136419b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T16:24:21.141282Z",
     "start_time": "2024-04-18T16:24:13.355031Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1min 11 : runexp parallel run algo sequentially\n",
    "# Usage example\n",
    "tsp_file_path = 'a29.tsp'\n",
    "csv_file_path = 'a29.csv'\n",
    "\n",
    "tsp_instance = ProcessData(tsp_file_path, csv_file_path)\n",
    "plotter = PrettyPlotting()\n",
    "# Run experiments for different inertia weights\n",
    "experiment = Experiment(tsp_instance, num_runs=2)\n",
    "inertia_weights = [0.4, 0.6, 0.8, 1.0]\n",
    "experiment.run_inertia_weight_experiments(inertia_weights)\n",
    "\n",
    "# Usage example\n",
    "tsp_file_path = 'a280.tsp'\n",
    "csv_file_path = 'a280_distance_matrix.csv'\n",
    "\n",
    "tsp_instance = ProcessData(tsp_file_path, csv_file_path)\n",
    "plotter = PrettyPlotting()\n",
    "# Run experiments for different inertia weights\n",
    "experiment = Experiment(tsp_instance, num_runs=2)\n",
    "inertia_weights = [0.4, 0.6, 0.8, 1.0]\n",
    "experiment.run_inertia_weight_experiments(inertia_weights)\n",
    "\n",
    "# Usage example\n",
    "tsp_file_path = 'fl1400.tsp'\n",
    "csv_file_path = 'fl1400_distance_matrix.csv'\n",
    "\n",
    "tsp_instance = ProcessData(tsp_file_path, csv_file_path)\n",
    "plotter = PrettyPlotting()\n",
    "# Run experiments for different inertia weights\n",
    "experiment = Experiment(tsp_instance, num_runs=2)\n",
    "inertia_weights = [0.4, 0.6, 0.8, 1.0]\n",
    "experiment.run_inertia_weight_experiments(inertia_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4489ac1343adc8",
   "metadata": {},
   "source": [
    "### 4.3. Experiment 3: Acceleration Coefficients\n",
    "\n",
    "#### Experiment Design:\n",
    "\n",
    "\n",
    "\n",
    "| Algorithm                       | Optimal Acceleration Coefficients | Comments                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
    "|---------------------------------|------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Particle Swarm Optimization (PSO) | 2.0, 1.5                           | PSO achieves the best performance with acceleration coefficients of 2.0 for the cognitive component and 1.5 for the social component. This combination strikes a balance between the particle's individual experience and the influence of the global best solution. This allows for an effective exploration and exploitation of the search space. The higher cognitive coefficient encourages particles to explore their own best positions, while the slightly lower social coefficient prevents excessive convergence to the global best.             |\n",
    "| Adaptive PSO (APSO)             | 1.5, 2.0                           | APSO performs optimally with acceleration coefficients of 1.5 for the cognitive component and 2.0 for the social component. The adaptive mechanism in APSO dynamically adjusts the inertia weight based on the search progress. The higher social coefficient in APSO promotes faster convergence towards the global best solution, while the lower cognitive coefficient maintains diversity and allows for exploration of new regions. This combination complements the adaptive inertia weight strategy in APSO. |\n",
    "| Discrete PSO (HPSO)             | 2.0, 1.5                           | HPSO exhibits similar behavior to standard PSO and performs best with acceleration coefficients of 2.0 for the cognitive component and 1.5 for the social component. The discrete nature of the TSP solution space requires a balance between exploration and exploitation. The higher cognitive coefficient in HPSO encourages particles to explore their own best positions in the discrete space, while the lower social coefficient prevents premature convergence to suboptimal solutions. |\n",
    "| Spatial PSO (SPSO)              | 2.0, 2.0                           | SPSO incorporates spatial information by considering the neighborhood of each particle. The optimal acceleration coefficients for SPSO are 2.0 for both the cognitive and social components. This equal emphasis on individual and neighborhood best positions allows SPSO to effectively exploit the local information within the particle's neighborhood while still maintaining a strong influence from the global best solution. The balanced coefficients facilitate thorough exploration of promising regions in the search space. |\n",
    "| Differential Evolution PSO (DEPSO) | 1.5, 2.0                           | DEPSO achieves the best performance with acceleration coefficients of 1.5 for the cognitive component and 2.0 for the social component. The differential evolution mechanism in DEPSO introduces additional exploration capabilities. The higher social coefficient promotes the influence of the global best solution. This allows DEPSO to converge faster towards promising regions. The lower cognitive coefficient maintains diversity and prevents premature convergence, complementing the exploration provided by the differential evolution operator. |\n",
    "| Predator-Prey PSO (PPPSO)       | 1.5, 2.0                           | PPPSO performs optimally with acceleration coefficients of 1.5 for the cognitive component and 2.0 for the social component. The predator-prey dynamics in PPPSO introduce an additional influence on the particles' movement. The higher social coefficient encourages particles to follow the global best solution more closely, while the lower cognitive coefficient allows for exploration and escape from the predator's influence. This combination enhances the algorithm's ability to navigate the search space effectively and avoid stagnation. |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f54f610c21b05b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T16:24:36.814769Z",
     "start_time": "2024-04-18T16:24:21.142744Z"
    }
   },
   "outputs": [],
   "source": [
    "# Usage example\n",
    "tsp_file_path = 'a29.tsp'\n",
    "csv_file_path = 'a29.csv'\n",
    "\n",
    "tsp_instance = ProcessData(tsp_file_path, csv_file_path)\n",
    "plotter = PrettyPlotting()\n",
    "# Run experiments for different acceleration coefficient combinations\n",
    "coefficient_combinations = [(1.5, 1.5), (1.5, 2.0), (2.0, 1.5), (2.0, 2.0)]\n",
    "experiment = Experiment(tsp_instance, num_runs=2)\n",
    "experiment.run_acceleration_coefficients_experiments(coefficient_combinations)\n",
    "\n",
    "# Usage example\n",
    "tsp_file_path = 'a280.tsp'\n",
    "csv_file_path = 'a280_distance_matrix.csv'\n",
    "\n",
    "tsp_instance = ProcessData(tsp_file_path, csv_file_path)\n",
    "plotter = PrettyPlotting()\n",
    "# Run experiments for different acceleration coefficient combinations\n",
    "coefficient_combinations = [(1.5, 1.5), (1.5, 2.0), (2.0, 1.5), (2.0, 2.0)]\n",
    "experiment = Experiment(tsp_instance, num_runs=2)\n",
    "experiment.run_acceleration_coefficients_experiments(coefficient_combinations)\n",
    "\n",
    "\n",
    "# Usage example\n",
    "tsp_file_path = 'fl1400.tsp'\n",
    "csv_file_path = 'fl1400_distance_matrix.csv'\n",
    "\n",
    "tsp_instance = ProcessData(tsp_file_path, csv_file_path)\n",
    "plotter = PrettyPlotting()\n",
    "# Run experiments for different acceleration coefficient combinations\n",
    "coefficient_combinations = [(1.5, 1.5), (1.5, 2.0), (2.0, 1.5), (2.0, 2.0)]\n",
    "experiment = Experiment(tsp_instance, num_runs=2)\n",
    "experiment.run_acceleration_coefficients_experiments(coefficient_combinations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9104639cc57e4dec",
   "metadata": {},
   "source": [
    "### 4.4. Experiment 4: w_min and w_max\n",
    "\n",
    "#### Experiment Design:\n",
    "\n",
    "\n",
    "| Algorithm             | Optimal w_min | Optimal w_max | Comments                                                                                                                                                                                                                                         |\n",
    "|-----------------------|---------------|---------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Adaptive PSO (APSO)   | 0.2           | 0.8           | APSO achieves the best performance with w_min of 0.2 and w_max of 0.8. The adaptive inertia weight mechanism in APSO allows for a dynamic balance between exploration and exploitation throughout the search process. The low w_min value of 0.2 encourages exploitation and convergence towards the end of the search, while the high w_max value of 0.8 promotes exploration in the early stages. This wide range of inertia weight values enables APSO to adapt its search behavior based on the progress and effectively navigate the TSP solution space. The optimal w_min and w_max combination strikes a balance between global exploration and local refinement. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9966cb45090c3b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T16:24:40.313332Z",
     "start_time": "2024-04-18T16:24:36.815527Z"
    }
   },
   "outputs": [],
   "source": [
    "# Usage example\n",
    "tsp_file_path = 'a29.tsp'\n",
    "csv_file_path = 'a29.csv'\n",
    "\n",
    "tsp_instance = ProcessData(tsp_file_path, csv_file_path)\n",
    "plotter = PrettyPlotting()\n",
    "# Run experiments for different w_min and w_max combinations (APSO)\n",
    "w_min_w_max_combinations = [(0.2, 0.8), (0.4, 0.9), (0.1, 0.7), (0.3, 0.6)]\n",
    "experiment = Experiment(tsp_instance, num_runs=2)\n",
    "experiment.run_w_min_w_max_experiments(w_min_w_max_combinations)\n",
    "\n",
    "# Usage example\n",
    "tsp_file_path = 'a280.tsp'\n",
    "csv_file_path = 'a280_distance_matrix.csv'\n",
    "\n",
    "tsp_instance = ProcessData(tsp_file_path, csv_file_path)\n",
    "plotter = PrettyPlotting()\n",
    "# Run experiments for different w_min and w_max combinations (APSO)\n",
    "w_min_w_max_combinations = [(0.2, 0.8), (0.4, 0.9), (0.1, 0.7), (0.3, 0.6)]\n",
    "experiment = Experiment(tsp_instance, num_runs=2)\n",
    "experiment.run_w_min_w_max_experiments(w_min_w_max_combinations)\n",
    "\n",
    "\n",
    "# Usage example\n",
    "tsp_file_path = 'fl1400.tsp'\n",
    "csv_file_path = 'fl1400_distance_matrix.csv'\n",
    "\n",
    "tsp_instance = ProcessData(tsp_file_path, csv_file_path)\n",
    "plotter = PrettyPlotting()\n",
    "# Run experiments for different w_min and w_max combinations (APSO)\n",
    "w_min_w_max_combinations = [(0.2, 0.8), (0.4, 0.9), (0.1, 0.7), (0.3, 0.6)]\n",
    "experiment = Experiment(tsp_instance, num_runs=2)\n",
    "experiment.run_w_min_w_max_experiments(w_min_w_max_combinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f129d969e133787b",
   "metadata": {},
   "source": [
    "### 4.5. Experiment 5: Neighborhood Size\n",
    "\n",
    "#### Experiment Design:\n",
    "\n",
    "| Algorithm           | Optimal Neighborhood Size | Comments                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
    "|---------------------|---------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Spatial PSO (SPSO) | 3 (larger instances), 9 (smaller instances) | SPSO incorporates spatial information by considering the neighborhood of each particle. The optimal neighborhood size depends on the size of the TSP instance. For larger instances, A neighborhood size of 3 proves to be effective. A smaller neighborhood size in larger instances allows for more focused local search and exploitation of promising regions which reduces the computational overhead. On the other hand, For smaller TSP instances, A larger neighborhood size of 9 is beneficial. The increased neighborhood size in smaller instances enables SPSO to explore a wider range of solutions and gather more information from neighboring particles. The adaptive choice of neighborhood size based on the instance size allows SPSO to strike a balance between local exploitation and global exploration. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e9e7bc27c12594",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T16:24:44.535343Z",
     "start_time": "2024-04-18T16:24:40.314342Z"
    }
   },
   "outputs": [],
   "source": [
    "# Usage example\n",
    "tsp_file_path = 'a29.tsp'\n",
    "csv_file_path = 'a29.csv'\n",
    "\n",
    "tsp_instance = ProcessData(tsp_file_path, csv_file_path)\n",
    "plotter = PrettyPlotting()\n",
    "# Run experiments for different neighborhood sizes (SPSO)\n",
    "neighborhood_sizes = [3, 5, 7, 9]\n",
    "experiment = Experiment(tsp_instance, num_runs=2)\n",
    "experiment.run_neighborhood_size_experiments(neighborhood_sizes)\n",
    "\n",
    "# Usage example\n",
    "tsp_file_path = 'a280.tsp'\n",
    "csv_file_path = 'a280_distance_matrix.csv'\n",
    "\n",
    "tsp_instance = ProcessData(tsp_file_path, csv_file_path)\n",
    "plotter = PrettyPlotting()\n",
    "# Run experiments for different neighborhood sizes (SPSO)\n",
    "neighborhood_sizes = [3, 5, 7, 9]\n",
    "experiment = Experiment(tsp_instance, num_runs=2)\n",
    "experiment.run_neighborhood_size_experiments(neighborhood_sizes)\n",
    "\n",
    "# Usage example\n",
    "tsp_file_path = 'fl1400.tsp'\n",
    "csv_file_path = 'fl1400_distance_matrix.csv'\n",
    "\n",
    "tsp_instance = ProcessData(tsp_file_path, csv_file_path)\n",
    "plotter = PrettyPlotting()\n",
    "# Run experiments for different neighborhood sizes (SPSO)\n",
    "neighborhood_sizes = [3, 5, 7, 9]\n",
    "experiment = Experiment(tsp_instance, num_runs=2)\n",
    "experiment.run_neighborhood_size_experiments(neighborhood_sizes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257832ed87eecc29",
   "metadata": {},
   "source": [
    "### 4.6. Experiment 6: Crossover Rate and Mutation Rate\n",
    "\n",
    "#### Experiment Design:\n",
    "\n",
    "| Algorithm                    | Optimal Crossover | Optimal Mutation Rate | Comments                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
    "|------------------------------|------------|------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Differential Evolution PSO (DEPSO) | 0.8        | 0.2        |  The optimal crossover rate (CR) of 0.8 and mutation rate (F) of 0.2 strike a balance between exploration and exploitation in DEPSO. The high CR value of 0.8 ensures that a significant portion of the candidate solution is inherited from the mutant vector which allows the preservation of good solution components. On the other hand, The low F value of 0.2 introduces a moderate level of mutation. This combination of CR and F values enables DEPSO to effectively navigate the TSP solution space. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee16c6e4d8912b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T16:24:52.977970Z",
     "start_time": "2024-04-18T16:24:44.536220Z"
    }
   },
   "outputs": [],
   "source": [
    "# Usage example\n",
    "tsp_file_path = 'a29.tsp'\n",
    "csv_file_path = 'a29.csv'\n",
    "\n",
    "tsp_instance = ProcessData(tsp_file_path, csv_file_path)\n",
    "plotter = PrettyPlotting()\n",
    "# Run experiments for different CR and F combinations (DEPSO)\n",
    "cr_f_combinations = [(0.5, 0.5), (0.8, 0.2), (0.2, 0.8), (0.9, 0.1)]\n",
    "experiment = Experiment(tsp_instance, num_runs=2)\n",
    "experiment.run_cr_f_experiments(cr_f_combinations)\n",
    "\n",
    "# Usage example\n",
    "tsp_file_path = 'a280.tsp'\n",
    "csv_file_path = 'a280_distance_matrix.csv'\n",
    "\n",
    "tsp_instance = ProcessData(tsp_file_path, csv_file_path)\n",
    "plotter = PrettyPlotting()\n",
    "# Run experiments for different CR and F combinations (DEPSO)\n",
    "cr_f_combinations = [(0.5, 0.5), (0.8, 0.2), (0.2, 0.8), (0.9, 0.1)]\n",
    "experiment = Experiment(tsp_instance, num_runs=2)\n",
    "experiment.run_cr_f_experiments(cr_f_combinations)\n",
    "\n",
    "# Usage example\n",
    "tsp_file_path = 'fl1400.tsp'\n",
    "csv_file_path = 'fl1400_distance_matrix.csv'\n",
    "\n",
    "tsp_instance = ProcessData(tsp_file_path, csv_file_path)\n",
    "plotter = PrettyPlotting()\n",
    "# Run experiments for different CR and F combinations (DEPSO)\n",
    "cr_f_combinations = [(0.5, 0.5), (0.8, 0.2), (0.2, 0.8), (0.9, 0.1)]\n",
    "experiment = Experiment(tsp_instance, num_runs=2)\n",
    "experiment.run_cr_f_experiments(cr_f_combinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b639ef163a3d3ba4",
   "metadata": {},
   "source": [
    "### 4.7. Experiment 7: Fear Factor\n",
    "\n",
    "#### Experiment Design:\n",
    "\n",
    "| Algorithm                    | Optimal Fear Factor | Comments                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
    "|------------------------------|----------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Predator-Prey PSO (PPPSO)   | 0.2 (smaller instances), 0.4 (larger instances) | The fear factor parameter controls the influence of the predator on the movement of the prey particles. The optimal fear factor value depends on the size of the TSP instance. For smaller instances, A fear factor of 0.2 is found to be effective. A lower fear factor in smaller instances allows the prey particles to focus more on local search and exploitation of promising regions, As the search space is relatively limited. The reduced influence of the predator enables the particles to converge quickly towards high-quality solutions. On the other hand, For larger TSP instances, A higher fear factor of 0.4 is beneficial. The increased fear factor in larger instances promotes exploration and helps the particles to escape from local optima. The stronger influence of the predator encourages the particles to explore a wider range of solutions and avoid premature convergence. The adaptive setting of the fear factor based on the instance size allows PPPSO to strike a balance between exploration and exploitation. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc20b407759368c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T16:24:56.725804Z",
     "start_time": "2024-04-18T16:24:52.978882Z"
    }
   },
   "outputs": [],
   "source": [
    "# Usage example\n",
    "tsp_file_path = 'a29.tsp'\n",
    "csv_file_path = 'a29.csv'\n",
    "\n",
    "tsp_instance = ProcessData(tsp_file_path, csv_file_path)\n",
    "plotter = PrettyPlotting()\n",
    "# Run experiments for different fear factors (PPPSO)\n",
    "fear_factors = [0.2, 0.4, 0.6, 0.8]\n",
    "experiment = Experiment(tsp_instance, num_runs=2)\n",
    "experiment.run_fear_factor_experiments(fear_factors)\n",
    "\n",
    "# Usage example\n",
    "tsp_file_path = 'a280.tsp'\n",
    "csv_file_path = 'a280_distance_matrix.csv'\n",
    "\n",
    "tsp_instance = ProcessData(tsp_file_path, csv_file_path)\n",
    "plotter = PrettyPlotting()\n",
    "# Run experiments for different fear factors (PPPSO)\n",
    "fear_factors = [0.2, 0.4, 0.6, 0.8]\n",
    "experiment = Experiment(tsp_instance, num_runs=2)\n",
    "experiment.run_fear_factor_experiments(fear_factors)\n",
    "\n",
    "# Usage example\n",
    "tsp_file_path = 'fl1400.tsp'\n",
    "csv_file_path = 'fl1400_distance_matrix.csv'\n",
    "\n",
    "tsp_instance = ProcessData(tsp_file_path, csv_file_path)\n",
    "plotter = PrettyPlotting()\n",
    "# Run experiments for different fear factors (PPPSO)\n",
    "fear_factors = [0.2, 0.4, 0.6, 0.8]\n",
    "experiment = Experiment(tsp_instance, num_runs=2)\n",
    "experiment.run_fear_factor_experiments(fear_factors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e5c2f595178daf",
   "metadata": {},
   "source": [
    "### 4.8. Experiment 8: Effect of number of iterations on the algorithms' best solution\n",
    "\n",
    "#### Experiment Design:\n",
    "\n",
    "| Algorithm               | Observations                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | Comments                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
    "|-------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Particle Swarm Optimization (PSO) | May improve larger instances or not improve in smaller instances                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | PSO's performance with respect to the maximum number of iterations varies depending on the size of the TSP instance. For larger instances, Increasing the maximum iterations may lead to improvement in solution quality, As PSO has more opportunities to explore the vast search space and refine its solutions. However, For smaller instances, Increasing the maximum iterations may not result in significant improvement, As PSO might have already converged to near-optimal solutions within a smaller number of iterations. The impact of increasing iterations on PSO's performance is more pronounced in larger instances, Where the algorithm benefits from additional exploration and refinement. |\n",
    "| Adaptive PSO (APSO)    | May improve larger instances or not improve in smaller instances                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | APSO like PSO, exhibits varying performance depending on the TSP instance size when increasing the maximum iterations. For larger instances, APSO may benefit from a higher number of iterations, as the adaptive inertia weight mechanism allows for a balance between exploration and exploitation over a longer search process. The additional iterations provide APSO with more opportunities to adapt its search behavior and discover high-quality solutions. However, for smaller instances, increasing the maximum iterations may not lead to substantial improvement, as APSO might have already converged to near-optimal solutions within a smaller number of iterations. The impact of increasing iterations on APSO's performance is more evident in larger instances, where the algorithm can leverage its adaptive capabilities effectively. |\n",
    "| Discrete PSO (HPSO)    | May improve larger instances or not improve in smaller instances                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | HPSO displays similar behavior to PSO and APSO when increasing the maximum iterations. For larger TSP instances, HPSO may benefit from a higher number of iterations, as it allows the algorithm to explore a wider range of discrete solutions and refine its search process. The additional iterations provide HPSO with more chances to discover high-quality solutions in the vast discrete search space. However, for smaller instances, increasing the maximum iterations may not result in significant improvement, as HPSO might have already converged to near-optimal solutions within a smaller number of iterations. The impact of increasing iterations on HPSO's performance is more noticeable in larger instances, where the algorithm can effectively explore the discrete solution space. |\n",
    "| Random Sampling         | Improvement with slow gradient                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | Random Sampling, being a simple algorithm that generates random solutions, exhibits a slow improvement gradient when increasing the maximum iterations. As Random Sampling does not involve any intelligent search strategy, the improvement in solution quality is gradual and relatively slow compared to other algorithms. Increasing the maximum iterations allows Random Sampling to explore more random solutions, but the improvement is limited by the lack of guided search or learning mechanisms. The slow gradient suggests that the algorithm requires a large number of iterations to achieve significant improvements, and the progress is more evident in larger instances where the solution space is vast. |\n",
    "| Stochastic Hill Climber | Drastic improvement over smaller instances and less drastic but good improvement over larger instances                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Stochastic Hill Climber, being a local search algorithm, shows significant improvement when increasing the maximum iterations, particularly in smaller TSP instances. The algorithm's iterative improvement approach allows it to quickly converge to high-quality solutions in smaller instances, where the search space is relatively limited. Increasing the iterations enables the Stochastic Hill Climber to explore more neighboring solutions and refine its search process, leading to drastic improvements. For larger instances, the improvement is less drastic but still substantial, as the algorithm benefits from additional iterations to navigate the larger search space and escape local optima. The good improvement gradient in larger instances suggests that the Stochastic Hill Climber can effectively explore and optimize solutions even in complex TSP instances. |\n",
    "| Spatial PSO (SPSO)     | Continual slow improvement                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | SPSO, which incorporates spatial information by considering the neighborhood of each particle, exhibits a continual but slow improvement when increasing the maximum iterations. The spatial information allows SPSO to explore the search space more effectively, but the improvement gradient is gradual. Increasing the iterations provides SPSO with more opportunities to refine its search process and exploit the local information within neighborhoods. However, the slow improvement gradient suggests that SPSO requires a relatively large number of iterations to achieve significant enhancements in solution quality. The continual improvement indicates that SPSO benefits from additional iterations, but the progress is steady rather than rapid. |\n",
    "| Predator-Prey PSO (PPPSO) | May improve smaller instances or not improve in larger instances                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | PPPSO, which introduces a predator-prey mechanism, shows varying performance when increasing the maximum iterations depending on the TSP instance size. For smaller instances, PPPSO may benefit from a higher number of iterations, as the predator-prey dynamics help in exploring the search space and escaping local optima. The additional iterations allow PPPSO to refine its search process and discover high-quality solutions in smaller instances. However, for larger instances, increasing the maximum iterations may not lead to significant improvement, as the predator-prey mechanism might have already guided the search towards promising regions within a smaller number of iterations. The impact of increasing iterations on PPPSO's performance is more evident in smaller instances, where the algorithm can effectively leverage the predator-prey dynamics. |\n",
    "| Differential Evolution PSO (DEPSO) | Constant good gradient on smaller instances and larger instances, very good gradient                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | DEPSO, which combines the concepts of Differential Evolution and PSO, exhibits a consistently good improvement gradient when increasing the maximum iterations, both in smaller and larger TSP instances. The integration of DE operators allows DEPSO to maintain a good balance between exploration and exploitation, enabling it to effectively navigate the search space and discover high-quality solutions. Increasing the iterations provides DEPSO with more opportunities to refine its search process and leverage the DE operators for generating improved solutions. The constant good gradient suggests that DEPSO benefits significantly from additional iterations, leading to steady and substantial improvements in solution quality. The very good gradient in larger instances highlights DEPSO's ability to effectively explore and optimize solutions even in complex TSP scenarios. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac2e525aca5baf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T16:26:35.555814Z",
     "start_time": "2024-04-18T16:24:56.726751Z"
    }
   },
   "outputs": [],
   "source": [
    "# Usage example\n",
    "tsp_file_path = 'a29.tsp'\n",
    "csv_file_path = 'a29.csv'\n",
    "\n",
    "tsp_instance = ProcessData(tsp_file_path, csv_file_path)\n",
    "plotter = PrettyPlotting()\n",
    "\n",
    "\n",
    "experiment = Experiment(tsp_instance, num_runs=2,population_size=25,max_iterations=10, w=0.9, c1=0.1,c2=0.1, w_min=0.1, w_max=0.3, neighborhood_size=2, cr=0.1, f=0.1, fear_factor=0.9)\n",
    "experiment.max_iterations_experiments([10,50,100,200])\n",
    "\n",
    "# Usage example\n",
    "tsp_file_path = 'a280.tsp'\n",
    "csv_file_path = 'a280_distance_matrix.csv'\n",
    "\n",
    "tsp_instance = ProcessData(tsp_file_path, csv_file_path)\n",
    "plotter = PrettyPlotting()\n",
    "\n",
    "\n",
    "experiment = Experiment(tsp_instance, num_runs=2,population_size=25,max_iterations=10, w=0.9, c1=0.1,c2=0.1, w_min=0.1, w_max=0.3, neighborhood_size=2, cr=0.1, f=0.1, fear_factor=0.9)\n",
    "experiment.max_iterations_experiments([10,50,100,200])\n",
    "\n",
    "# Usage example\n",
    "tsp_file_path = 'fl1400.tsp'\n",
    "csv_file_path = 'fl1400_distance_matrix.csv'\n",
    "\n",
    "tsp_instance = ProcessData(tsp_file_path, csv_file_path)\n",
    "plotter = PrettyPlotting()\n",
    "\n",
    "\n",
    "experiment = Experiment(tsp_instance, num_runs=2,population_size=25,max_iterations=10, w=0.9, c1=0.1,c2=0.1, w_min=0.1, w_max=0.3, neighborhood_size=2, cr=0.1, f=0.1, fear_factor=0.9)\n",
    "experiment.max_iterations_experiments([10,50,100,200])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f431371b",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "We have successfully explored the effects of population size, inertia weight, acceleration coefficients, adaptive inertia weight strategies, spatial information integration, differential evolution operators, and predator-prey dynamics on the performance of PSO and its variants. We have also compared their performance against benchmark algorithms such as Random Sampling and Stochastic Hill Climber.\n",
    "\n",
    "Before we discuss the experiment results it is important to discuss few disclaimers, edge cases that can affect the conclusions we derive from this empirical investigation:\n",
    "\n",
    "  - The time complexity analysis is highly specific to this implementation of PSO, a new implementation with a  specially designed data structure for tsp for eg: splay trees can yield different results.\n",
    "  - We have evaluated 4 options for each hyperparameter based on intuition about the hyperparameter behaviour.\n",
    "   However, hyperparameter optimisation in itself is a growing field where we consider a large search space. \n",
    "   \n",
    "  - The computational cost has been prioritsed when setting up these experiments. A more comprehensive study would require more wall-clock as well as compute time.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The experimental results have yielded valuable insights into the behavior and effectiveness of each algorithm. We observed that the optimal population size varied among the algorithms, with PSO and its variants generally benefiting from larger population sizes compared to the benchmark algorithms. The choice of inertia weight significantly influenced the performance of PSO, HPSO, and SPSO, with higher inertia weights leading to better solution quality in PSO and HPSO, while lower inertia weights proved effective for SPSO.\n",
    "The acceleration coefficients played a crucial role in balancing exploration and exploitation in PSO and its variants. The optimal combination of cognitive and social acceleration coefficients varied among the algorithms, highlighting the importance of tuning these parameters for each specific variant. APSO, which incorporates an adaptive inertia weight strategy, demonstrated the impact of the range of inertia weights on its performance.\n",
    "\n",
    "The incorporation of spatial data in SPSO demonstrated encouraging outcomes, where the algorithm's efficiency was affected by the neighborhood size relative to the TSP instance size. DEPSO, which combines differential evolution operators with PSO, exhibited a consistently good improvement gradient across different instance sizes. PPPSO, with its predator-prey dynamics, showcased the importance of the fear factor parameter on its performance, with different optimal values for smaller and larger instances.\n",
    "\n",
    "Furthermore, the examination of the maximum number of iterations provided insights into the convergence behavior and computational requirements of each algorithm. The benchmark algorithms, Random Sampling and Stochastic Hill Climber, exhibited contrasting improvement gradients, with the latter showing drastic improvements, especially in smaller instances. PSO and its variants demonstrated varying improvements based on the instance size, with DEPSO standing out for its consistent and significant improvements across different scenarios.\n",
    "\n",
    "The study has also highlighted the computational complexity of each algorithm, with DEPSO exhibiting a higher time complexity compared to other PSO variants due to the additional differential evolution operations. The runtime analysis have provided a comprehensive understanding of the trade-offs between solution quality and computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10ad884cce6da75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T16:26:36.793930Z",
     "start_time": "2024-04-18T16:26:35.557142Z"
    }
   },
   "outputs": [],
   "source": [
    "# Usage example\n",
    "tsp_file_path = 'a29.tsp'\n",
    "csv_file_path = 'a29.csv'\n",
    "\n",
    "tsp_instance = ProcessData(tsp_file_path, csv_file_path)\n",
    "plotter = PrettyPlotting()\n",
    "\n",
    "\n",
    "experiment = Experiment(tsp_instance, num_runs=2,population_size=20,max_iterations=100, w=1.0, c1=2.0,c2=2.0, w_min=0.2, w_max=0.8, neighborhood_size=3, cr=0.8, f=0.1, fear_factor=0.4)\n",
    "experiment.run_best_hyperparameter_experiments()\n",
    "\n",
    "\n",
    "# Usage example\n",
    "tsp_file_path = 'a280.tsp'\n",
    "csv_file_path = 'a280_distance_matrix.csv'\n",
    "\n",
    "tsp_instance = ProcessData(tsp_file_path, csv_file_path)\n",
    "plotter = PrettyPlotting()\n",
    "\n",
    "\n",
    "experiment = Experiment(tsp_instance, num_runs=2,population_size=25,max_iterations=100, w=1.0, c1=2.0,c2=2.0, w_min=0.2, w_max=0.8, neighborhood_size=3, cr=0.8, f=0.2, fear_factor=0.4)\n",
    "experiment.run_best_hyperparameter_experiments()\n",
    "\n",
    "# Usage example\n",
    "tsp_file_path = 'fl1400.tsp'\n",
    "csv_file_path = 'fl1400_distance_matrix.csv'\n",
    "\n",
    "tsp_instance = ProcessData(tsp_file_path, csv_file_path)\n",
    "plotter = PrettyPlotting()\n",
    "\n",
    "\n",
    "experiment = Experiment(tsp_instance, num_runs=2,population_size=25,max_iterations=100, w=1.0, c1=2.0,c2=2.0, w_min=0.2, w_max=0.8, neighborhood_size=3, cr=0.8, f=0.2, fear_factor=0.4)\n",
    "experiment.run_best_hyperparameter_experiments()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c18545873345be8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T16:26:36.796952Z",
     "start_time": "2024-04-18T16:26:36.794858Z"
    }
   },
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "print(\"Execution time: \", end_time - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
